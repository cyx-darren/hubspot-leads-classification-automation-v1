#!/usr/bin/env python
"""
Lead Source Attribution Analysis

This script analyzes lead sources to determine whether they came from 
SEO, Google Ads PPC, direct traffic, or referrals.
"""

import os
import re
import io
import logging
import datetime2
import warnings
from collections import defaultdict
from dateutil import parser

# Configure logging first
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger('lead_attribution')

# Function to handle dependency imports
def import_dependencies():
    missing_deps = []

    # Try importing required packages
    try:
        global pd
        import pandas as pd
    except ImportError:
        missing_deps.append('pandas')

    try:
        global np
        import numpy as np
    except ImportError:
        missing_deps.append('numpy')

    try:
        global plt
        import matplotlib.pyplot as plt
    except ImportError:
        missing_deps.append('matplotlib')

    try:
        global sns
        import seaborn as sns
    except ImportError:
        missing_deps.append('seaborn')

    try:
        global PyPDF2
        import PyPDF2
    except ImportError:
        missing_deps.append('PyPDF2')

    try:
        global fuzz, process
        from fuzzywuzzy import fuzz, process
    except ImportError:
        missing_deps.append('fuzzywuzzy')

    # If there are missing dependencies, provide installation instructions
    if missing_deps:
        logger.error(f"Missing dependencies: {', '.join(missing_deps)}")
        logger.error("Please install the required packages using:")
        logger.error(f"pip install {' '.join(missing_deps)}")
        logger.error("Or use the provided requirements.txt file:")
        logger.error("pip install -r requirements.txt")
        return False

    return True

# Import dependencies
dependencies_installed = import_dependencies()

class LeadAttributionAnalyzer:
    def __init__(self):
        self.leads_df = None
        self.customers_df = None
        self.seo_keywords_df = None
        self.ppc_standard_df = None
        self.ppc_dynamic_df = None
        self.combined_ppc_df = None
        self.product_keyword_map = None
        self.attribution_window_hours = 48
        self.confidence_thresholds = {
            'high': 80,
            'medium': 50,
            'low': 20
        }

    def load_data(self, 
                 leads_path, 
                 customers_path, 
                 seo_pdf_path, 
                 ppc_standard_path, 
                 ppc_dynamic_path):
        """Load all data sources"""
        logger.info("Loading data sources...")

        # Load leads data
        self.leads_df = pd.read_csv(leads_path)
        logger.info(f"Loaded {len(self.leads_df)} leads")

        # Load customer data - special handling since it has unusual structure
        try:
            # First check if the file exists
            if not os.path.exists(customers_path):
                logger.error(f"Customer data file not found: {customers_path}")
                self.customers_df = pd.DataFrame(columns=['email'])
            else:
                # Based on the inspection, Customers.xlsx has a single column with emails
                # but the header is itself an email address

                # Try reading without header first
                self.customers_df = pd.read_excel(customers_path, header=None)

                # If it has only one column, rename it to 'email'
                if len(self.customers_df.columns) == 1:
                    self.customers_df.columns = ['email']
                    logger.info(f"Loaded {len(self.customers_df)} customer records (single column)")
                else:
                    # Otherwise try reading normally
                    self.customers_df = pd.read_excel(customers_path)
                    logger.info(f"Loaded {len(self.customers_df)} customer records (multi-column)")
        except Exception as e:
            logger.error(f"Error loading customers data: {e}")
            # Create empty DataFrame as fallback
            self.customers_df = pd.DataFrame(columns=['email'])

        # Load SEO data from PDF
        self.seo_keywords_df = self.extract_seo_data_from_pdf(seo_pdf_path)
        logger.info(f"Extracted {len(self.seo_keywords_df)} SEO keywords")

        # Load PPC data
        try:
            self.ppc_standard_df = pd.read_csv(ppc_standard_path)
            self.ppc_dynamic_df = pd.read_csv(ppc_dynamic_path)
            logger.info(f"Loaded {len(self.ppc_standard_df)} standard PPC records and {len(self.ppc_dynamic_df)} dynamic PPC records")
        except Exception as e:
            logger.error(f"Error loading PPC data: {e}")
            raise

        # Process and clean the data
        self.process_data()

    def extract_seo_data_from_pdf(self, file_path):
        """Extract SEO keyword data from file (now CSV)"""
        try:
            # Check if file exists and has CSV extension
            if os.path.exists(file_path) and file_path.endswith('.csv'):
                logger.info(f"Reading SEO data from CSV: {file_path}")

                # Read the CSV file
                seo_df = pd.read_csv(file_path)

                # Rename columns to match expected format if needed
                if 'Keyphrase' in seo_df.columns and 'keyphrase' not in seo_df.columns:
                    seo_df = seo_df.rename(columns={'Keyphrase': 'keyphrase'})

                if 'Current Position' in seo_df.columns:
                    seo_df = seo_df.rename(columns={'Current Position': 'current_position'})

                # Convert position to numeric
                seo_df['current_position'] = pd.to_numeric(seo_df['current_position'], errors='coerce')

                # Fill NaN values
                seo_df['current_position'] = seo_df['current_position'].fillna(100)  # Assume low rank for missing values

                # Add product category based on keyphrase
                seo_df['product_category'] = seo_df['keyphrase'].apply(self.extract_product_category_from_keyword)

                logger.info(f"Loaded {len(seo_df)} SEO keywords from CSV")

                return seo_df
            else:
                # Fall back to previous mock data if CSV doesn't exist
                logger.warning(f"CSV file not found: {file_path}. Using mock data instead.")
                return self.create_mock_seo_data()

        except Exception as e:
            logger.error(f"Error extracting SEO data from CSV: {e}")
            # Return empty DataFrame as fallback
            return pd.DataFrame()

    def extract_product_category(self, url):
        """Extract product category from URL"""
        # Map URLs to product categories based on patterns
        if 'wooden-coaster' in url:
            return 'coasters'
        elif 'cotton-socks' in url:
            return 'socks'
        elif 'wristbands' in url:
            return 'wristbands'
        elif 'corporate-gifts/o' in url:
            return 'office_supplies'
        elif 'stress-balls' in url:
            return 'stress_balls'
        elif 'gift-bo' in url:
            return 'gift_boxes'
        elif 'water-bo' in url:
            return 'water_bottles'
        elif 'arm-sleeves' in url:
            return 'arm_sleeves'
        elif 'safety-vest' in url:
            return 'safety_vests'
        else:
            return 'other'

    def extract_product_category_from_keyword(self, keyword):
        """Extract product category from keyword text"""
        if not isinstance(keyword, str):
            return 'other'

        keyword = keyword.lower()

        # Check each product category's keywords for matches
        for category, terms in self.product_keyword_map.items():
            for term in terms:
                if term in keyword:
                    return category

        return 'other'

    def process_data(self):
        """Process and clean all data sources"""
        logger.info("Processing and cleaning data...")

        # Create product-keyword mapping first (needed by extract_product_category_from_keyword)
        self.create_product_keyword_mapping()

        # Process leads data
        self.process_leads_data()

        # Process customer data
        self.process_customer_data()

        # Process SEO data
        # (Already processed in extract_seo_data_from_pdf)

        # Process and combine PPC data
        self.process_ppc_data()

    def process_leads_data(self):
        """Process and clean leads data"""
        # First, inspect the columns
        logger.info(f"Lead data columns: {list(self.leads_df.columns)}")

        # Find email column
        email_column = None
        possible_email_columns = ['email', 'Email', 'EMAIL', 'email_address', 'Email Address', 'EmailAddress']

        for col in possible_email_columns:
            if col in self.leads_df.columns:
                email_column = col
                logger.info(f"Found lead email column: '{col}'")
                break

        # If we don't find a standard email column, check content
        if email_column is None:
            for col in self.leads_df.columns:
                # Skip if column is already identified as something else or is numeric
                if col in ['first_inquiry_timestamp', 'product', 'ticket_id', 'subject', 'total_jan_tickets']:
                    continue

                # Sample some values to check if they look like emails
                sample_values = self.leads_df[col].dropna().head(5).astype(str)
                email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
                email_matches = [bool(re.match(email_pattern, val)) for val in sample_values]

                if any(email_matches):
                    email_column = col
                    logger.info(f"Identified likely lead email column: '{col}' based on content")
                    break

        # If we still can't find an email column, use a placeholder
        if email_column is None:
            logger.warning("No email column found in lead data. Attribution based on email matching won't be possible.")
            self.leads_df['email'] = 'unknown@example.com'
            email_column = 'email'
        else:
            # Clean email addresses
            self.leads_df[email_column] = self.leads_df[email_column].astype(str).str.lower().str.strip()

            # Create a standard 'email' column if it doesn't exist
            if email_column != 'email':
                self.leads_df['email'] = self.leads_df[email_column]

        # Handle timestamp column
        timestamp_column = None
        possible_timestamp_columns = ['first_inquiry_timestamp', 'timestamp', 'date', 'inquiry_date', 'Date', 'Time']

        for col in possible_timestamp_columns:
            if col in self.leads_df.columns:
                timestamp_column = col
                logger.info(f"Found timestamp column: '{col}'")
                break

        if timestamp_column is None:
            logger.warning("No timestamp column found. Using current date for all leads.")
            self.leads_df['first_inquiry_timestamp'] = datetime.datetime.now()
        else:
            # Convert timestamp to datetime
            try:
                self.leads_df[timestamp_column] = pd.to_datetime(self.leads_df[timestamp_column])
                # Create a standard 'first_inquiry_timestamp' column if it doesn't exist
                if timestamp_column != 'first_inquiry_timestamp':
                    self.leads_df['first_inquiry_timestamp'] = self.leads_df[timestamp_column]
            except:
                # If standard parsing fails, try different formats
                try:
                    self.leads_df[timestamp_column] = self.leads_df[timestamp_column].apply(
                        lambda x: parser.parse(str(x)) if not pd.isna(x) else pd.NaT
                    )
                    # Create a standard column
                    if timestamp_column != 'first_inquiry_timestamp':
                        self.leads_df['first_inquiry_timestamp'] = self.leads_df[timestamp_column]
                except Exception as e:
                    logger.warning(f"Could not parse timestamps: {e}. Using current date for all leads.")
                    self.leads_df['first_inquiry_timestamp'] = datetime.datetime.now()

        # Find product and subject columns
        product_column = 'product' if 'product' in self.leads_df.columns else None
        subject_column = 'subject' if 'subject' in self.leads_df.columns else None

        # Extract keywords from subject if available
        if subject_column:
            self.leads_df['extracted_keywords'] = self.leads_df[subject_column].apply(self.extract_keywords_from_subject)
        else:
            # Try to find another column that might contain product information
            possible_product_cols = [col for col in self.leads_df.columns if 'product' in col.lower() or 'item' in col.lower() or 'desc' in col.lower()]

            if possible_product_cols:
                alternative_col = possible_product_cols[0]
                logger.info(f"Using '{alternative_col}' as alternative to subject for keyword extraction")
                self.leads_df['extracted_keywords'] = self.leads_df[alternative_col].apply(self.extract_keywords_from_subject)
            else:
                logger.warning("No subject or product description column found. Keyword extraction will be limited.")
                self.leads_df['extracted_keywords'] = [[]] * len(self.leads_df)

        # Initialize attribution columns
        self.leads_df['attributed_source'] = 'Unknown'
        self.leads_df['attribution_confidence'] = 0
        self.leads_df['attribution_detail'] = ''

        # Extract day of week and hour for temporal analysis
        self.leads_df['day_of_week'] = self.leads_df['first_inquiry_timestamp'].dt.day_name()
        self.leads_df['hour_of_day'] = self.leads_df['first_inquiry_timestamp'].dt.hour

        logger.info("Leads data processed")

    def extract_keywords_from_subject(self, subject):
        """Extract potential keywords from email subject"""
        if not isinstance(subject, str):
            return []

        # Convert to lowercase
        subject = subject.lower()

        # Common product terms to look for
        product_terms = [
            'coaster', 'coasters', 'socks', 'wristband', 'wristbands', 'notebook', 'notebooks',
            'stress ball', 'stress balls', 'gift box', 'water bottle', 'shirt', 'shirts',
            'arm sleeve', 'arm sleeves', 'vest', 'lanyard', 'lanyards', 'pen', 'pens'
        ]

        # Extract words
        words = re.findall(r'\b\w+\b', subject)

        # Find product terms in subject
        found_terms = []
        for term in product_terms:
            if term in subject:
                found_terms.append(term)

        # Extract 2-3 word phrases that might be keywords
        phrases = []
        for i in range(len(words)-1):
            phrases.append(f"{words[i]} {words[i+1]}")
            if i < len(words)-2:
                phrases.append(f"{words[i]} {words[i+1]} {words[i+2]}")

        return found_terms + phrases

    def process_customer_data(self):
        """Process and clean customer data"""
        # Check if the customers dataframe has the email column
        if 'email' not in self.customers_df.columns:
            logger.warning("No 'email' column found in customer data. Looking for alternative columns.")

            # Log available columns
            logger.info(f"Available columns in customer data: {list(self.customers_df.columns)}")

            # If it's a single column dataframe, assume it contains emails
            if len(self.customers_df.columns) == 1:
                # Rename the single column to 'email'
                original_column = self.customers_df.columns[0]
                self.customers_df.rename(columns={original_column: 'email'}, inplace=True)
                logger.info(f"Renamed column '{original_column}' to 'email'")

        # Clean email addresses
        self.customers_df['email'] = self.customers_df['email'].astype(str).str.lower().str.strip()

        # Filter out invalid emails
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        valid_mask = self.customers_df['email'].str.match(email_pattern)

        # Filter to only include valid emails
        valid_emails = self.customers_df.loc[valid_mask, 'email'].dropna().unique()

        # Create set of customer emails for faster lookup
        self.customer_emails = set(valid_emails)

        logger.info(f"Customer data processed. Found {len(self.customer_emails)} unique valid customer emails")

    def process_ppc_data(self):
        """Process and combine PPC campaign data"""
        # Process standard campaign data
        try:
            # Get the actual column names from the DataFrame
            std_cols = list(self.ppc_standard_df.columns)
            logger.info(f"Standard PPC columns: {std_cols}")

            # Map expected column positions to actual names
            day_col = std_cols[0]  # Should be 'Day'
            keyword_col = std_cols[1]  # Should be 'Search keyword'
            clicks_col = std_cols[2]  # Should be 'Clicks'
            impressions_col = std_cols[3]  # Should be 'Impr.'

            # Rename columns for consistency
            self.ppc_standard_df = self.ppc_standard_df.rename(columns={
                day_col: 'date',
                keyword_col: 'keyword',
                clicks_col: 'clicks',
                impressions_col: 'impressions'
            })

            # Convert date - handle various formats
            try:
                self.ppc_standard_df['date'] = pd.to_datetime(self.ppc_standard_df['date'], format='%d/%m/%y')
            except:
                try:
                    self.ppc_standard_df['date'] = pd.to_datetime(self.ppc_standard_df['date'], format='%d/%m/%Y')
                except:
                    try:
                        self.ppc_standard_df['date'] = pd.to_datetime(self.ppc_standard_df['date'])
                    except Exception as e:
                        logger.warning(f"Could not parse dates in standard PPC data: {e}")
                        # Create a dummy date
                        self.ppc_standard_df['date'] = pd.to_datetime('2025-01-01')

            # Convert numeric columns
            self.ppc_standard_df['clicks'] = pd.to_numeric(self.ppc_standard_df['clicks'], errors='coerce')
            self.ppc_standard_df['impressions'] = pd.to_numeric(self.ppc_standard_df['impressions'], errors='coerce')

            # Add campaign type
            self.ppc_standard_df['campaign_type'] = 'Standard'

        except Exception as e:
            logger.error(f"Error processing standard PPC data: {e}")
            import traceback
            logger.error(traceback.format_exc())

        # Process dynamic campaign data
        try:
            # Get the actual column names from the DataFrame
            dyn_cols = list(self.ppc_dynamic_df.columns)
            logger.info(f"Dynamic PPC columns: {dyn_cols}")

            # Map expected column positions to actual names
            day_col = dyn_cols[0]  # Should be 'Day'
            target_col = dyn_cols[1]  # Should be 'Dynamic ad target'
            clicks_col = dyn_cols[2]  # Should be 'Clicks'
            impressions_col = dyn_cols[3]  # Should be 'Impr.'

            # Rename columns for consistency
            self.ppc_dynamic_df = self.ppc_dynamic_df.rename(columns={
                day_col: 'date',
                target_col: 'keyword',
                clicks_col: 'clicks',
                impressions_col: 'impressions'
            })

            # Convert date - handle various formats
            try:
                self.ppc_dynamic_df['date'] = pd.to_datetime(self.ppc_dynamic_df['date'], format='%d/%m/%y')
            except:
                try:
                    self.ppc_dynamic_df['date'] = pd.to_datetime(self.ppc_dynamic_df['date'], format='%d/%m/%Y')
                except:
                    try:
                        self.ppc_dynamic_df['date'] = pd.to_datetime(self.ppc_dynamic_df['date'])
                    except Exception as e:
                        logger.warning(f"Could not parse dates in dynamic PPC data: {e}")
                        # Create a dummy date
                        self.ppc_dynamic_df['date'] = pd.to_datetime('2025-01-01')

            # Convert numeric columns
            self.ppc_dynamic_df['clicks'] = pd.to_numeric(self.ppc_dynamic_df['clicks'], errors='coerce')
            self.ppc_dynamic_df['impressions'] = pd.to_numeric(self.ppc_dynamic_df['impressions'], errors='coerce')

            # Add campaign type
            self.ppc_dynamic_df['campaign_type'] = 'Dynamic'

        except Exception as e:
            logger.error(f"Error processing dynamic PPC data: {e}")
            import traceback
            logger.error(traceback.format_exc())

        # Combine PPC data
        try:
            common_columns = ['date', 'keyword', 'clicks', 'impressions', 'campaign_type']

            # Check if both DataFrames are not empty
            if not self.ppc_standard_df.empty and not self.ppc_dynamic_df.empty:
                self.combined_ppc_df = pd.concat([
                    self.ppc_standard_df[common_columns],
                    self.ppc_dynamic_df[common_columns]
                ]).reset_index(drop=True)
            elif not self.ppc_standard_df.empty:
                self.combined_ppc_df = self.ppc_standard_df[common_columns].copy()
            elif not self.ppc_dynamic_df.empty:
                self.combined_ppc_df = self.ppc_dynamic_df[common_columns].copy()
            else:
                self.combined_ppc_df = pd.DataFrame(columns=common_columns)

            # Add day of week for temporal analysis
            self.combined_ppc_df['day_of_week'] = self.combined_ppc_df['date'].dt.day_name()
            self.combined_ppc_df['hour_of_day'] = 0  # We don't have hour-level data for PPC

            logger.info("PPC data combined and processed")
        except Exception as e:
            logger.error(f"Error combining PPC data: {e}")
            import traceback
            logger.error(traceback.format_exc())
            self.combined_ppc_df = pd.DataFrame()

    def create_product_keyword_mapping(self):
        """Create mapping between products and keywords"""
        # This is a simplified mapping - in a real implementation, this would be more comprehensive
        self.product_keyword_map = {
            'coasters': ['coaster', 'coasters', 'wood coaster', 'wooden coaster', 'drink coaster'],
            'socks': ['sock', 'socks', 'custom socks', 'logo socks', 'custom sock'],
            'stress_balls': ['stress ball', 'stress balls', 'smiley stress', 'corporate stress'],
            'wristbands': ['wristband', 'wristbands', 'branded wristband', 'silicone wristband'],
            'notebooks': ['notebook', 'notebooks', 'custom notebook', 'printed notebook'],
            'gift_boxes': ['gift box', 'gift boxes', 'custom gift box', 'printed gift box'],
            'water_bottles': ['water bottle', 'bottle', 'bottles', 'foldable bottle'],
            'arm_sleeves': ['arm sleeve', 'arm sleeves', 'custom arm sleeve'],
            'safety_vests': ['vest', 'vests', 'safety vest', 'hi vis vest', 'high vis'],
            'lanyards': ['lanyard', 'lanyards', 'custom lanyard', 'printed lanyard']
        }

        logger.info("Product-keyword mapping created")

    def run_attribution(self):
        """Run the full attribution process"""
        logger.info("Starting attribution process...")

        # Step 1: Identify direct traffic (returning customers)
        self.identify_direct_traffic()

        # Step 2: Identify SEO traffic (moved before PPC)
        self.identify_seo_traffic()

        # Step 3: Identify potential referrals
        self.identify_referrals()

        # Step 4: Identify PPC traffic (moved to last)
        self.identify_ppc_traffic()

        # Step 5: Calculate confidence scores and finalize attribution
        self.finalize_attribution()

        logger.info("Attribution process completed")

        return self.leads_df

    def identify_direct_traffic(self):
        """Identify direct traffic from returning customers"""
        logger.info("Identifying direct traffic...")

        # Check if each lead email is in the customer list
        direct_mask = self.leads_df['email'].isin(self.customer_emails)

        # Mark direct traffic
        self.leads_df.loc[direct_mask, 'attributed_source'] = 'Direct'
        self.leads_df.loc[direct_mask, 'attribution_confidence'] = 100
        self.leads_df.loc[direct_mask, 'attribution_detail'] = 'Returning customer'

        direct_count = direct_mask.sum()
        logger.info(f"Identified {direct_count} leads as direct traffic ({direct_count/len(self.leads_df)*100:.1f}%)")

    def identify_ppc_traffic(self):
        """Identify traffic from PPC campaigns"""
        logger.info("Identifying PPC traffic...")

        if self.combined_ppc_df.empty:
            logger.warning("No PPC data available - skipping PPC attribution")
            return

        # Only consider leads not already attributed
        unattributed_mask = self.leads_df['attributed_source'] == 'Unknown'

        # Count PPC attributions
        ppc_count = 0

        # Loop through unattributed leads
        for idx, lead in self.leads_df[unattributed_mask].iterrows():
            lead_time = lead['first_inquiry_timestamp']

            # Define time window for attribution
            time_window_start = lead_time - datetime.timedelta(hours=self.attribution_window_hours)

            # Convert date objects to pandas Timestamp for comparison
            time_window_start_date = pd.Timestamp(time_window_start.date())
            lead_time_date = pd.Timestamp(lead_time.date())

            # Find PPC clicks within time window - using pandas Timestamps for comparison
            ppc_clicks_in_window = self.combined_ppc_df[
                (self.combined_ppc_df['date'] >= time_window_start_date) & 
                (self.combined_ppc_df['date'] <= lead_time_date) & 
                (self.combined_ppc_df['clicks'] > 0)
            ]

            if ppc_clicks_in_window.empty:
                continue

            # Extract lead keywords from subject or product field
            lead_keywords = []
            if not pd.isna(lead['product']):
                lead_keywords.extend(self.extract_keywords_from_text(lead['product']))
            if not pd.isna(lead['subject']):
                lead_keywords.extend(self.extract_keywords_from_text(lead['subject']))
            lead_keywords = list(set(lead_keywords))  # Remove duplicates

            # Match lead keywords with PPC keywords
            keyword_match_score = 0
            matched_keywords = []

            for ppc_idx, ppc_click in ppc_clicks_in_window.iterrows():
                ppc_keyword = ppc_click['keyword'].lower()
                ppc_keyword_terms = self.extract_keywords_from_text(ppc_keyword)

                for lead_kw in lead_keywords:
                    for ppc_kw in ppc_keyword_terms:
                        similarity = fuzz.token_sort_ratio(lead_kw, ppc_kw)
                        if similarity > 60:  # 70% similarity threshold
                            keyword_match_score = max(keyword_match_score, similarity)
                            matched_keywords.append((lead_kw, ppc_kw, similarity))

            # Calculate time proximity score (closer is better)
            time_proximity_score = 0
            if not ppc_clicks_in_window.empty:
                # Get most recent click before lead
                lead_date = pd.Timestamp(lead_time.date())
                ppc_clicks_in_window['date_diff'] = (lead_date - ppc_clicks_in_window['date']).dt.days
                most_recent_click = ppc_clicks_in_window.loc[ppc_clicks_in_window['date_diff'].idxmin()]
                days_diff = most_recent_click['date_diff']

                # Higher score for more recent clicks
                if days_diff == 0:
                    time_proximity_score = 100
                elif days_diff == 1:
                    time_proximity_score = 90
                else:
                    time_proximity_score = max(0, 100 - (days_diff * 15))

            # Calculate overall PPC confidence score
            if keyword_match_score > 0 and time_proximity_score > 0:
                # Weighted average: 60% keyword match, 40% time proximity
                confidence_score = (0.6 * keyword_match_score) + (0.4 * time_proximity_score)

                # Attribute as PPC if confidence is high enough
                if confidence_score >= self.confidence_thresholds['low']:
                    self.leads_df.loc[idx, 'attributed_source'] = 'PPC'
                    self.leads_df.loc[idx, 'attribution_confidence'] = confidence_score

                    # Create detailed attribution info
                    matched_kw_str = '; '.join([f"{l}-{p}({s}%)" for l, p, s in matched_keywords[:3]])
                    detail = f"Keyword matches: {matched_kw_str}, Time proximity: {time_proximity_score:.1f}%"
                    self.leads_df.loc[idx, 'attribution_detail'] = detail

                    ppc_count += 1

        if unattributed_mask.sum() > 0:
            logger.info(f"Identified {ppc_count} leads as PPC traffic ({ppc_count/unattributed_mask.sum()*100:.1f}% of unattributed)")
        else:
            logger.info("No unattributed leads to analyze for PPC")

    def extract_keywords_from_text(self, text):
        """Extract keywords from text string"""
        if not isinstance(text, str):
            return []

        # Convert to lowercase
        text = text.lower()

        # Extract words and phrases
        words = re.findall(r'\b\w+\b', text)

        # Return individual words and 2-3 word phrases
        result = words.copy()
        for i in range(len(words)-1):
            result.append(f"{words[i]} {words[i+1]}")
            if i < len(words)-2:
                result.append(f"{words[i]} {words[i+1]} {words[i+2]}")

        return result

    def identify_seo_traffic(self):
        """Identify traffic from SEO"""
        logger.info("Identifying SEO traffic...")

        if self.seo_keywords_df.empty:
            logger.warning("No SEO data available - skipping SEO attribution")
            return

        # Only consider leads not already attributed
        unattributed_mask = self.leads_df['attributed_source'] == 'Unknown'

        # Count SEO attributions
        seo_count = 0

        # Loop through unattributed leads
        for idx, lead in self.leads_df[unattributed_mask].iterrows():
            lead_time = lead['first_inquiry_timestamp']

            # Extract lead keywords from subject or product field
            lead_keywords = []
            if not pd.isna(lead['product']):
                lead_keywords.extend(self.extract_keywords_from_text(lead['product']))
            if not pd.isna(lead['subject']):
                lead_keywords.extend(self.extract_keywords_from_text(lead['subject']))
            lead_keywords = list(set(lead_keywords))  # Remove duplicates

            # Match lead keywords with SEO keywords
            keyword_match_score = 0
            matched_keywords = []
            matched_positions = []

            for seo_idx, seo_kw in self.seo_keywords_df.iterrows():
                seo_keyword = seo_kw['keyphrase'].lower()
                seo_keyword_terms = self.extract_keywords_from_text(seo_keyword)

                for lead_kw in lead_keywords:
                    for seo_kw_term in seo_keyword_terms:
                        similarity = fuzz.token_sort_ratio(lead_kw, seo_kw_term)
                        if similarity > 60:  # 70% similarity threshold
                            # Higher score for better rankings
                            position_bonus = max(0, 10 - seo_kw['current_position']) * 3
                            adjusted_score = similarity + position_bonus
                            keyword_match_score = max(keyword_match_score, adjusted_score)
                            matched_keywords.append((lead_kw, seo_kw_term, similarity))
                            matched_positions.append(seo_kw['current_position'])

            # Check if we have matching keywords with good rankings
            if keyword_match_score > 0:
                # Create a position-adjusted score (higher for better rankings)
                position_score = 0
                if matched_positions:
                    avg_position = sum(matched_positions) / len(matched_positions)
                    if avg_position <= 1:
                        position_score = 100
                    elif avg_position <= 3:
                        position_score = 90
                    elif avg_position <= 5:
                        position_score = 80
                    elif avg_position <= 10:
                        position_score = 70
                    else:
                        position_score = 60

                # Calculate overall SEO confidence score
                # Weighted average: 70% keyword match, 30% position score
                confidence_score = (0.7 * keyword_match_score) + (0.3 * position_score)

                # Normalize to 0-100
                confidence_score = min(100, confidence_score)

                # Attribute as SEO if confidence is high enough
                if confidence_score >= self.confidence_thresholds['low']:
                    self.leads_df.loc[idx, 'attributed_source'] = 'SEO'
                    self.leads_df.loc[idx, 'attribution_confidence'] = confidence_score

                    # Create detailed attribution info
                    matched_kw_str = '; '.join([f"{l}-{s}({p})" for l, s, p in list(zip(
                        [m[0] for m in matched_keywords[:3]], 
                        [m[2] for m in matched_keywords[:3]], 
                        matched_positions[:3]
                    ))])
                    detail = f"Keyword matches: {matched_kw_str}, Avg position: {sum(matched_positions)/len(matched_positions):.1f}"
                    self.leads_df.loc[idx, 'attribution_detail'] = detail

                    seo_count += 1

        if unattributed_mask.sum() > 0:
            logger.info(f"Identified {seo_count} leads as SEO traffic ({seo_count/unattributed_mask.sum()*100:.1f}% of unattributed)")
        else:
            logger.info("No unattributed leads to analyze for SEO")

    def identify_referrals(self):
        """Identify potential referral traffic"""
        logger.info("Identifying potential referrals...")

        # Only consider leads not already attributed
        unattributed_mask = self.leads_df['attributed_source'] == 'Unknown'

        # 1. Look for email domain patterns (multiple leads from same company)
        # Extract email domains
        self.leads_df['email_domain'] = self.leads_df['email'].apply(
            lambda x: x.split('@')[1] if isinstance(x, str) and '@' in x else ''
        )

        # Count emails per domain
        domain_counts = self.leads_df['email_domain'].value_counts()
        multiple_lead_domains = domain_counts[domain_counts > 1].index.tolist()

        # 2. Look for temporal clusters
        # Get timestamp for reference
        if not self.leads_df.empty and 'first_inquiry_timestamp' in self.leads_df.columns:
            self.leads_df['inquiry_date'] = self.leads_df['first_inquiry_timestamp'].dt.date
            date_counts = self.leads_df['inquiry_date'].value_counts()
            busy_dates = date_counts[date_counts > 2].index.tolist()
        else:
            busy_dates = []

        # Count referral attributions
        referral_count = 0

        # Identify potential referrals
        for idx, lead in self.leads_df[unattributed_mask].iterrows():
            referral_score = 0
            referral_evidence = []

            # Check domain pattern
            if lead['email_domain'] in multiple_lead_domains:
                domain_count = domain_counts[lead['email_domain']]
                # Higher score for more emails from same domain
                domain_score = min(60, domain_count * 15)
                referral_score += domain_score
                referral_evidence.append(f"Domain pattern: {domain_count} leads from {lead['email_domain']}")

            # Check temporal clusters
            if hasattr(lead['first_inquiry_timestamp'], 'date') and lead['first_inquiry_timestamp'].date() in busy_dates:
                # Get number of inquiries on this date
                date_count = date_counts[lead['first_inquiry_timestamp'].date()]

                # Find inquiries within 3 hours of this one
                inquiry_time = lead['first_inquiry_timestamp']
                time_window_start = inquiry_time - datetime.timedelta(hours=3)
                time_window_end = inquiry_time + datetime.timedelta(hours=3)

                # Count inquiries in time window
                time_window_inquiries = self.leads_df[
                    (self.leads_df['first_inquiry_timestamp'] >= time_window_start) &
                    (self.leads_df['first_inquiry_timestamp'] <= time_window_end) &
                    (self.leads_df.index != idx)  # Exclude current inquiry
                ]

                time_cluster_count = len(time_window_inquiries)
                if time_cluster_count > 0:
                    # Higher score for more inquiries in time window
                    time_score = min(40, time_cluster_count * 10)
                    referral_score += time_score
                    referral_evidence.append(f"Time cluster: {time_cluster_count} leads within 3 hours")

            # Calculate overall referral confidence score
            confidence_score = min(100, referral_score)

            # Attribute as Referral if confidence is high enough
            if confidence_score >= self.confidence_thresholds['low']:
                self.leads_df.loc[idx, 'attributed_source'] = 'Referral'
                self.leads_df.loc[idx, 'attribution_confidence'] = confidence_score
                self.leads_df.loc[idx, 'attribution_detail'] = '; '.join(referral_evidence)

                referral_count += 1

        if unattributed_mask.sum() > 0:
            logger.info(f"Identified {referral_count} leads as Referral traffic ({referral_count/unattributed_mask.sum()*100:.1f}% of unattributed)")
        else:
            logger.info("No unattributed leads to analyze for Referrals")

    def finalize_attribution(self):
        """Finalize attribution and set confidence levels"""
        # Categorize confidence levels
        self.leads_df['confidence_level'] = self.leads_df['attribution_confidence'].apply(
            lambda score: 'High' if score >= self.confidence_thresholds['high'] else 
                         ('Medium' if score >= self.confidence_thresholds['medium'] else 
                          ('Low' if score >= self.confidence_thresholds['low'] else 'Unknown'))
        )

        # Count final attribution by source
        attribution_counts = self.leads_df['attributed_source'].value_counts()

        logger.info("Final attribution summary:")
        for source, count in attribution_counts.items():
            logger.info(f"  {source}: {count} leads ({count/len(self.leads_df)*100:.1f}%)")

        # Count by confidence level
        confidence_counts = self.leads_df['confidence_level'].value_counts()

        logger.info("Attribution confidence summary:")
        for level, count in confidence_counts.items():
            logger.info(f"  {level}: {count} leads ({count/len(self.leads_df)*100:.1f}%)")

    def generate_analysis(self, output_dir='./output'):
        """Generate analysis and visualizations"""
        logger.info("Generating analysis and visualizations...")

        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)

        # 1. Save attributed leads to CSV
        output_file = os.path.join(output_dir, 'leads_with_attribution.csv')
        self.leads_df.to_csv(output_file, index=False)
        logger.info(f"Saved attributed leads to {output_file}")

        # 2. Generate source distribution chart
        self.generate_source_distribution_chart(output_dir)

        # 3. Generate confidence level chart
        self.generate_confidence_chart(output_dir)

        # 4. Generate temporal analysis
        self.generate_temporal_analysis(output_dir)

        # 5. Generate product analysis
        self.generate_product_analysis(output_dir)

        # 6. Generate attribution summary report
        self.generate_summary_report(output_dir)

        logger.info("Analysis and visualization generation completed")

    def generate_source_distribution_chart(self, output_dir):
        """Generate chart showing distribution of traffic sources"""
        try:
            plt.figure(figsize=(10, 6))
            source_counts = self.leads_df['attributed_source'].value_counts()

            # Create pie chart
            plt.subplot(1, 2, 1)
            plt.pie(source_counts, labels=source_counts.index, autopct='%1.1f%%', startangle=90)
            plt.axis('equal')
            plt.title('Lead Attribution by Source')

            # Create bar chart
            plt.subplot(1, 2, 2)
            ax = sns.barplot(x=source_counts.index, y=source_counts.values)

            # Add value labels on bars
            for i, v in enumerate(source_counts.values):
                ax.text(i, v + 0.5, str(v), ha='center')

            plt.title('Lead Count by Source')
            plt.xlabel('Source')
            plt.ylabel('Number of Leads')
            plt.xticks(rotation=45)

            plt.tight_layout()

            # Save chart
            output_file = os.path.join(output_dir, 'source_distribution.png')
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            plt.close()

            logger.info(f"Saved source distribution chart to {output_file}")
        except Exception as e:
            logger.error(f"Error generating source distribution chart: {e}")

    def generate_confidence_chart(self, output_dir):
        """Generate chart showing attribution confidence levels"""
        try:
            plt.figure(figsize=(12, 6))

            # Create stacked bar chart of confidence levels by source
            confidence_by_source = pd.crosstab(
                self.leads_df['attributed_source'], 
                self.leads_df['confidence_level']
            )

            # Ensure all confidence levels are present
            for level in ['High', 'Medium', 'Low', 'Unknown']:
                if level not in confidence_by_source.columns:
                    confidence_by_source[level] = 0

            # Order columns by confidence level
            confidence_by_source = confidence_by_source[['High', 'Medium', 'Low', 'Unknown']]

            # Create stacked bar chart
            ax = confidence_by_source.plot(kind='bar', stacked=True, figsize=(12, 6), 
                                  color=['#2ecc71', '#3498db', '#f1c40f', '#e74c3c'])

            plt.title('Attribution Confidence by Source')
            plt.xlabel('Source')
            plt.ylabel('Number of Leads')
            plt.legend(title='Confidence Level')

            # Add value labels on bars
            for c in ax.containers:
                labels = [int(v) if v > 0 else '' for v in c.datavalues]
                ax.bar_label(c, labels=labels, label_type='center')

            plt.tight_layout()

            # Save chart
            output_file = os.path.join(output_dir, 'confidence_levels.png')
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            plt.close()

            logger.info(f"Saved confidence level chart to {output_file}")
        except Exception as e:
            logger.error(f"Error generating confidence chart: {e}")

    def generate_temporal_analysis(self, output_dir):
        """Generate temporal analysis visualizations"""
        try:
            # 1. Day of week analysis
            plt.figure(figsize=(14, 6))

            # Order days of week correctly
            day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

            # Count leads by day of week and source
            day_source_counts = pd.crosstab(self.leads_df['day_of_week'], self.leads_df['attributed_source'])

            # Reindex to ensure correct order
            day_source_counts = day_source_counts.reindex(day_order)

            # Create stacked bar chart
            ax = day_source_counts.plot(kind='bar', stacked=True, figsize=(14, 6))

            plt.title('Lead Distribution by Day of Week and Source')
            plt.xlabel('Day of Week')
            plt.ylabel('Number of Leads')
            plt.legend(title='Source')

            # Save chart
            output_file = os.path.join(output_dir, 'day_of_week_analysis.png')
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            plt.close()

            # 2. Hour of day analysis
            plt.figure(figsize=(16, 6))

            # Count leads by hour of day and source
            hour_source_counts = pd.crosstab(self.leads_df['hour_of_day'], self.leads_df['attributed_source'])

            # Create stacked bar chart
            ax = hour_source_counts.plot(kind='bar', stacked=True, figsize=(16, 6))

            plt.title('Lead Distribution by Hour of Day and Source')
            plt.xlabel('Hour of Day (24-hour format)')
            plt.ylabel('Number of Leads')
            plt.legend(title='Source')
            plt.xticks(range(24))

            # Save chart
            output_file = os.path.join(output_dir, 'hour_of_day_analysis.png')
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            plt.close()

            logger.info(f"Saved temporal analysis charts to {output_dir}")
        except Exception as e:
            logger.error(f"Error generating temporal analysis: {e}")

    def generate_product_analysis(self, output_dir):
        """Generate product-based analysis"""
        try:
            # Use product column if available
            if 'product' in self.leads_df.columns and not self.leads_df['product'].isna().all():
                product_field = 'product'
            else:
                # Try to extract products from subject
                self.leads_df['extracted_product'] = self.leads_df['subject'].apply(
                    lambda x: self.identify_product_from_text(x) if isinstance(x, str) else 'Unknown'
                )
                product_field = 'extracted_product'

            # Count leads by product and source
            product_source_counts = pd.crosstab(
                self.leads_df[product_field], 
                self.leads_df['attributed_source']
            )

            # Create stacked bar chart
            plt.figure(figsize=(14, 8))
            ax = product_source_counts.plot(kind='barh', stacked=True, figsize=(14, 8))

            plt.title('Lead Distribution by Product and Source')
            plt.xlabel('Number of Leads')
            plt.ylabel('Product')
            plt.legend(title='Source')

            # Save chart
            output_file = os.path.join(output_dir, 'product_analysis.png')
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            plt.close()

            logger.info(f"Saved product analysis chart to {output_file}")
        except Exception as e:
            logger.error(f"Error generating product analysis: {e}")

    def identify_product_from_text(self, text):
        """Identify product category from text"""
        if not isinstance(text, str):
            return 'Unknown'

        text = text.lower()

        for product, keywords in self.product_keyword_map.items():
            for keyword in keywords:
                if keyword in text:
                    # Convert from code to display name
                    return product.replace('_', ' ').title()

        return 'Unknown'

    def generate_summary_report(self, output_dir):
        """Generate summary report with key insights"""
        try:
            # Prepare summary data
            total_leads = len(self.leads_df)
            source_counts = self.leads_df['attributed_source'].value_counts()
            confidence_counts = self.leads_df['confidence_level'].value_counts()

            # Prepare source percentages
            source_pcts = source_counts / total_leads * 100

            # Create summary report
            report = [
                "# Lead Source Attribution Analysis Summary",
                f"Generated on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                f"Total leads analyzed: {total_leads}",
                "",
                "## Attribution by Source",
                "| Source | Count | Percentage |",
                "| ------ | ----- | ---------- |"
            ]

            # Add source rows
            for source in source_counts.index:
                report.append(f"| {source} | {source_counts[source]} | {source_pcts[source]:.1f}% |")

            report.extend([
                "",
                "## Attribution Confidence",
                "| Confidence | Count | Percentage |",
                "| ---------- | ----- | ---------- |"
            ])

            # Add confidence rows
            for level in confidence_counts.index:
                pct = confidence_counts[level] / total_leads * 100
                report.append(f"| {level} | {confidence_counts[level]} | {pct:.1f}% |")

            # Add key insights
            report.extend([
                "",
                "## Key Insights",
                ""
            ])

            # Add source-specific insights
            if 'Direct' in source_counts:
                direct_pct = source_pcts['Direct'] if 'Direct' in source_pcts else 0
                report.append(f"- **Returning Customers**: {direct_pct:.1f}% of leads came from returning customers.")

            if 'PPC' in source_counts:
                ppc_pct = source_pcts['PPC'] if 'PPC' in source_pcts else 0
                report.append(f"- **PPC Performance**: {ppc_pct:.1f}% of leads were attributed to PPC campaigns.")

            if 'SEO' in source_counts:
                seo_pct = source_pcts['SEO'] if 'SEO' in source_pcts else 0
                report.append(f"- **SEO Performance**: {seo_pct:.1f}% of leads were attributed to organic search traffic.")

            if 'Referral' in source_counts:
                referral_pct = source_pcts['Referral'] if 'Referral' in source_pcts else 0
                report.append(f"- **Referrals**: {referral_pct:.1f}% of leads likely came from referrals.")

            if 'Unknown' in source_counts:
                unknown_pct = source_pcts['Unknown'] if 'Unknown' in source_pcts else 0
                report.append(f"- **Unattributed Traffic**: {unknown_pct:.1f}% of leads could not be attributed with confidence.")

            # Add temporal insights
            if 'day_of_week' in self.leads_df.columns:
                day_counts = self.leads_df['day_of_week'].value_counts()
                top_day = day_counts.index[0] if not day_counts.empty else 'Unknown'
                report.append(f"- **Peak Day**: {top_day} was the most active day for lead generation.")

            if 'hour_of_day' in self.leads_df.columns:
                hour_counts = self.leads_df['hour_of_day'].value_counts()
                top_hour = hour_counts.index[0] if not hour_counts.empty else 'Unknown'
                report.append(f"- **Peak Hour**: {top_hour}:00-{top_hour+1}:00 was the most active hour for lead generation.")

            # Write report to file
            report_text = '\n'.join(report)
            output_file = os.path.join(output_dir, 'attribution_summary.md')

            with open(output_file, 'w') as f:
                f.write(report_text)

            logger.info(f"Saved summary report to {output_file}")
        except Exception as e:
            logger.error(f"Error generating summary report: {e}")


def main():
    """Main function to run the lead attribution analysis"""
    # Check if dependencies are installed
    if not dependencies_installed:
        return

    try:
        # Initialize the analyzer
        analyzer = LeadAttributionAnalyzer()

        # Load data - Updated to use Feb2025-SEO.csv
        analyzer.load_data(
            leads_path="leads_analysis 2.csv",
            customers_path="Customers.xlsx",
            seo_pdf_path="Feb2025-SEO.csv",  # Changed from Jan2025-SEO.csv to Feb2025-SEO.csv
            ppc_standard_path="When your ads showed Custom and Corporate Gifts and Lanyards.csv",
            ppc_dynamic_path="When your ads showed Dynamic Search Ads.csv"
        )

        # Run attribution
        attributed_leads = analyzer.run_attribution()

        # Generate analysis and visualizations
        analyzer.generate_analysis(output_dir="./output")

        logger.info("Lead attribution analysis completed successfully")

        return attributed_leads

    except Exception as e:
        logger.error(f"Error in lead attribution analysis: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise


if __name__ == "__main__":
    main()